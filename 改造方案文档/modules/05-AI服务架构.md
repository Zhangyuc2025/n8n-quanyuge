# AI æœåŠ¡æ¶æ„

> **ç‰ˆæœ¬ï¼š** v1.0
> **çŠ¶æ€ï¼š** è®¾è®¡ä¸­

[â† è¿”å›æ€»è§ˆ](../00-æ€»è§ˆä¸å¯¼èˆª.md)

---

## ğŸ¤– AI æœåŠ¡æ¶æ„

### LmChatPlatform èŠ‚ç‚¹ï¼ˆå·²å®ç°ï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `packages/@n8n/nodes-langchain/nodes/llms/LmChatPlatform/LmChatPlatform.node.ts`

**æ ¸å¿ƒç‰¹æ€§ï¼š**
1. âœ… é€šç”¨èŠ‚ç‚¹ï¼Œé€šè¿‡éšè—å‚æ•°åŒºåˆ†æä¾›å•†
2. âœ… åŠ¨æ€åŠ è½½æ¨¡å‹åˆ—è¡¨ï¼ˆä»åå° APIï¼‰
3. âœ… æ— éœ€å‡­è¯ï¼Œå¹³å°æ‰˜ç®¡
4. âœ… è‡ªåŠ¨è®¡è´¹

**èŠ‚ç‚¹å‚æ•°ï¼š**

```typescript
properties: [
  {
    // éšè—å­—æ®µï¼šæä¾›å•†æ ‡è¯†
    displayName: 'Provider Key',
    name: 'providerKey',
    type: 'hidden',
    default: '',  // å‰ç«¯è®¾ç½®ä¸º 'openai', 'anthropic' ç­‰
  },
  {
    // éšè—å­—æ®µï¼šæä¾›å•†åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    displayName: 'Provider Name',
    name: 'providerName',
    type: 'hidden',
    default: '',  // å‰ç«¯è®¾ç½®ä¸º 'OpenAI', 'Anthropic' ç­‰
  },
  {
    // éšè—å­—æ®µï¼šæä¾›å•†å›¾æ ‡
    displayName: 'Provider Icon',
    name: 'providerIcon',
    type: 'hidden',
    default: '',
  },
  {
    // æ¨¡å‹é€‰æ‹©ï¼ˆåŠ¨æ€åŠ è½½ï¼‰
    displayName: 'Model',
    name: 'model',
    type: 'resourceLocator',
    default: { mode: 'list', value: '' },
    required: true,
    modes: [
      {
        displayName: 'From List',
        name: 'list',
        type: 'list',
        typeOptions: {
          searchListMethod: 'searchModels',  // â† å…³é”®ï¼šåŠ¨æ€åŠ è½½
        },
      },
    ],
  },
  // ... å…¶ä»–å‚æ•°ï¼ˆtemperature, maxTokens ç­‰ï¼‰
]
```

**åŠ¨æ€åŠ è½½æ¨¡å‹åˆ—è¡¨ï¼š**

```typescript
methods = {
  listSearch: {
    async searchModels(this: ISupplyDataFunctions) {
      const providerKey = this.getNodeParameter('providerKey', 0) as string;

      if (!providerKey) {
        return { results: [] };
      }

      try {
        // âœ… è°ƒç”¨åå° API è·å–æ¨¡å‹åˆ—è¡¨
        const response = await this.helpers.httpRequest({
          method: 'GET',
          url: `${this.getRestApiUrl()}/platform-ai-providers/${providerKey}/models`,
          json: true,
        });

        const models = Array.isArray(response) ? response : [];

        return {
          results: models.map((model: any) => ({
            name: `${model.name} (Â¥${model.pricePerToken}/1K tokens)`,
            value: model.id,
            description: model.description,
          })),
        };
      } catch (error) {
        console.error('Failed to load models:', error);
        return { results: [] };
      }
    },
  },
};
```

**æ‰§è¡Œæ—¶è°ƒç”¨åå°ï¼š**

```typescript
async supplyData(this: ISupplyDataFunctions, itemIndex: number): Promise<SupplyData> {
  const providerKey = this.getNodeParameter('providerKey', itemIndex) as string;
  const modelId = this.getNodeParameter('model', itemIndex, '', { extractValue: true }) as string;
  const options = this.getNodeParameter('options', itemIndex, {}) as any;

  // âœ… åˆ›å»º PlatformChatModel å®ä¾‹
  const chatModel = new PlatformChatModel({
    providerKey,
    modelId,
    apiUrl: this.getRestApiUrl(),
    temperature: options.temperature ?? 0.7,
    maxTokens: options.maxTokens,
    httpRequestHelper: this.helpers.httpRequest.bind(this.helpers),
  });

  return { response: chatModel };
}
```

**PlatformChatModel ç±»ï¼š**

```typescript
class PlatformChatModel extends SimpleChatModel {
  async _call(messages: BaseMessage[]): Promise<string> {
    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const formattedMessages = messages.map((message) => ({
      role: this.mapMessageRole(message._getType()),
      content: message.content as string,
    }));

    try {
      // âœ… è°ƒç”¨åå°ç»Ÿä¸€ä»£ç†æ¥å£
      const response = await this.httpRequestHelper({
        method: 'POST',
        url: `${this.apiUrl}/platform-ai-providers/${this.providerKey}/chat/completions`,
        body: {
          model: this.modelId,
          messages: formattedMessages,
          temperature: this.temperature,
          maxTokens: this.maxTokens,
          // ...
        },
        json: true,
      });

      if (response.choices && response.choices.length > 0) {
        return response.choices[0].message.content;
      }

      throw new Error('No response from AI provider');
    } catch (error) {
      throw new Error(`Failed to call platform AI service: ${error.message}`);
    }
  }

  private mapMessageRole(messageType: string): string {
    const roleMap = {
      human: 'user',
      ai: 'assistant',
      system: 'system',
    };
    return roleMap[messageType] || 'user';
  }
}
```

### åå° API è®¾è®¡

#### 1. è·å–æ¨¡å‹åˆ—è¡¨

```typescript
// GET /platform-ai-providers/:providerKey/models

@Get('/:providerKey/models')
async getModels(@Param('providerKey') providerKey: string) {
  const provider = await this.platformAIService.getProvider(providerKey);

  if (!provider) {
    throw new NotFoundError('Provider not found');
  }

  // è¿”å› models_config
  return provider.modelsConfig.map(model => ({
    id: model.id,
    name: model.name,
    description: model.description,
    pricePerToken: model.pricePerToken,
    contextWindow: model.contextWindow,
    maxOutputTokens: model.maxOutputTokens,
  }));
}
```

#### 2. æ‰§è¡ŒèŠå¤©

```typescript
// POST /platform-ai-providers/:providerKey/chat/completions

@Post('/:providerKey/chat/completions')
async chat(
  @Param('providerKey') providerKey: string,
  @Body() body: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    temperature?: number;
    maxTokens?: number;
  },
  @WorkspaceContext() context: { workspaceId: string; userId: string }
) {
  // 1. è·å–æä¾›å•†é…ç½®
  const provider = await this.platformAIService.getProvider(providerKey);

  // 2. è§£å¯† API Key
  const apiConfig = this.cipher.decrypt(provider.apiConfig);

  // 3. è°ƒç”¨ä¸Šæ¸¸ API
  const response = await this.callUpstreamAPI(provider, apiConfig, body);

  // 4. è®¡ç®—è´¹ç”¨
  const tokensUsed = response.usage.total_tokens;
  const model = provider.modelsConfig.find(m => m.id === body.model);
  const cost = tokensUsed * model.pricePerToken / 1000;

  // 5. æ‰£è´¹
  await this.billingService.charge({
    workspaceId: context.workspaceId,
    userId: context.userId,
    serviceKey: `${providerKey}:${body.model}`,
    tokensUsed,
    cost,
  });

  // 6. è¿”å›å“åº”
  return response;
}

private async callUpstreamAPI(provider: any, apiConfig: any, body: any) {
  // æ ¹æ®æä¾›å•†ç±»å‹è°ƒç”¨ä¸åŒçš„ API
  switch (provider.providerKey) {
    case 'openai':
      return await this.callOpenAI(apiConfig, body);
    case 'anthropic':
      return await this.callAnthropic(apiConfig, body);
    default:
      throw new Error(`Unsupported provider: ${provider.providerKey}`);
  }
}

private async callOpenAI(apiConfig: any, body: any) {
  const response = await fetch(`${apiConfig.baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiConfig.apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: body.model,
      messages: body.messages,
      temperature: body.temperature ?? 0.7,
      max_tokens: body.maxTokens,
    }),
  });

  return await response.json();
}
```

---


[â† ä¸Šä¸€ç« ï¼šèŠ‚ç‚¹æ¶æ„](./04-èŠ‚ç‚¹æ¶æ„.md) | [è¿”å›æ€»è§ˆ](../00-æ€»è§ˆä¸å¯¼èˆª.md) | [ä¸‹ä¸€ç« ï¼šåç«¯å®ç° â†’](./06-åç«¯å®ç°.md)
